{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis and comparison of different ML models for earthquake prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path().parent / 'src'))\n",
    "\n",
    "from prediction_engine import EarthquakePredictionEngine\n",
    "from utils.model_evaluator import EarthquakeModelEvaluator\n",
    "from utils.visualization import EarthquakeVisualization\n",
    "from models.random_forest_model import EnhancedRandomForestModel\n",
    "from models.xgboost_model import XGBoostModel\n",
    "from models.neural_network_model import EnhancedNeuralNetworkModel\n",
    "from models.svm_model import SVMEarthquakeModel\n",
    "from models.ensemble_model import EarthquakeEnsembleModel\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the prediction engine\n",
    "engine = EarthquakePredictionEngine()\n",
    "\n",
    "# Collect and prepare data\n",
    "print(\"Collecting earthquake data...\")\n",
    "raw_data = engine.data_collector.fetch_enhanced_data(days_back=180, min_magnitude=4.0)\n",
    "print(f\"Collected {len(raw_data)} earthquake records\")\n",
    "\n",
    "# Feature engineering\n",
    "print(\"Engineering features...\")\n",
    "feature_data = engine.feature_engineer.create_all_features(raw_data)\n",
    "print(f\"Created {len(feature_data.columns)} features\")\n",
    "\n",
    "# Preprocessing\n",
    "X, y = engine.data_preprocessor.fit_transform(feature_data, target_col='magnitude')\n",
    "X_train, X_test, y_train, y_test = engine.data_preprocessor.create_train_test_split(X, y, temporal_split=False)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Individual Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = EarthquakeModelEvaluator()\n",
    "\n",
    "# Dictionary to store all results\n",
    "model_results = {}\n",
    "\n",
    "# Get feature names for analysis\n",
    "feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest...\")\n",
    "rf_model = EnhancedRandomForestModel()\n",
    "rf_training_results = rf_model.train(X_train, y_train, optimize_params=False)\n",
    "\n",
    "# Evaluate\n",
    "rf_results = evaluator.evaluate_model(\n",
    "    rf_model, X_train, X_test, y_train, y_test, \n",
    "    model_name=\"Random Forest\", feature_names=feature_names\n",
    ")\n",
    "\n",
    "model_results['Random Forest'] = rf_results\n",
    "\n",
    "print(f\"Random Forest R²: {rf_results['test_metrics']['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost...\")\n",
    "xgb_model = XGBoostModel()\n",
    "xgb_training_results = xgb_model.train(X_train, y_train, optimize_params=False)\n",
    "\n",
    "# Evaluate\n",
    "xgb_results = evaluator.evaluate_model(\n",
    "    xgb_model, X_train, X_test, y_train, y_test, \n",
    "    model_name=\"XGBoost\", feature_names=feature_names\n",
    ")\n",
    "\n",
    "model_results['XGBoost'] = xgb_results\n",
    "\n",
    "print(f\"XGBoost R²: {xgb_results['test_metrics']['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training SVM...\")\n",
    "svm_model = SVMEarthquakeModel()\n",
    "svm_training_results = svm_model.train(X_train, y_train, optimize_params=False)\n",
    "\n",
    "# Evaluate\n",
    "svm_results = evaluator.evaluate_model(\n",
    "    svm_model, X_train, X_test, y_train, y_test, \n",
    "    model_name=\"SVM\"\n",
    ")\n",
    "\n",
    "model_results['SVM'] = svm_results\n",
    "\n",
    "print(f\"SVM R²: {svm_results['test_metrics']['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Ensemble...\")\n",
    "ensemble_model = EarthquakeEnsembleModel()\n",
    "ensemble_model.initialize_models(['random_forest', 'xgboost', 'svm'])\n",
    "\n",
    "ensemble_training_results = ensemble_model.train(\n",
    "    X_train, y_train,\n",
    "    ensemble_type='weighted_average',\n",
    "    optimize_weights=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "ensemble_results = evaluator.evaluate_model(\n",
    "    ensemble_model, X_train, X_test, y_train, y_test, \n",
    "    model_name=\"Ensemble\"\n",
    ")\n",
    "\n",
    "model_results['Ensemble'] = ensemble_results\n",
    "\n",
    "print(f\"Ensemble R²: {ensemble_results['test_metrics']['r2']:.4f}\")\n",
    "print(f\"Ensemble weights: {ensemble_training_results['ensemble_weights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model comparison\n",
    "comparison = evaluator.compare_models(model_results)\n",
    "\n",
    "print(\"Model Comparison Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if 'overall_ranking' in comparison:\n",
    "    print(\"Overall Ranking (by Test R²):\")\n",
    "    for i, model_info in enumerate(comparison['overall_ranking']):\n",
    "        print(f\"{i+1}. {model_info['model']}: R² = {model_info['score']:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "for model_name, results in model_results.items():\n",
    "    if 'error' not in results:\n",
    "        metrics = results['test_metrics']\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  R²: {metrics['r2']:.4f}\")\n",
    "        print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "        print(f\"  MAE: {metrics['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = EarthquakeVisualization()\n",
    "\n",
    "# Model comparison plot\n",
    "valid_results = {k: v for k, v in model_results.items() if 'error' not in v}\n",
    "if len(valid_results) >= 2:\n",
    "    comparison_fig = visualizer.plot_model_comparison(valid_results)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 valid models for comparison plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for tree-based models\n",
    "if 'Random Forest' in model_results and 'feature_importance' in model_results['Random Forest']:\n",
    "    rf_importance = model_results['Random Forest']['feature_importance']['top_10_features']\n",
    "    rf_importance_df = pd.DataFrame(rf_importance)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    importance_fig = visualizer.plot_feature_importance(rf_importance_df, top_n=15)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 Most Important Features (Random Forest):\")\n",
    "    for i, feat in enumerate(rf_importance[:10]):\n",
    "        print(f\"{i+1:2d}. {feat['feature']:<25} {feat['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the best model\n",
    "best_model_name = None\n",
    "best_r2 = -999\n",
    "\n",
    "for name, results in model_results.items():\n",
    "    if 'error' not in results:\n",
    "        r2 = results['test_metrics']['r2']\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model_name = name\n",
    "\n",
    "if best_model_name:\n",
    "    print(f\"Best model: {best_model_name} (R² = {best_r2:.4f})\")\n",
    "    \n",
    "    # Get the best model object\n",
    "    if best_model_name == 'Random Forest':\n",
    "        best_model = rf_model\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        best_model = xgb_model\n",
    "    elif best_model_name == 'SVM':\n",
    "        best_model = svm_model\n",
    "    elif best_model_name == 'Ensemble':\n",
    "        best_model = ensemble_model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Get uncertainties if available\n",
    "    uncertainties = None\n",
    "    if hasattr(best_model, 'predict_with_uncertainty'):\n",
    "        _, uncertainties = best_model.predict_with_uncertainty(X_test)\n",
    "    \n",
    "    # Plot prediction results\n",
    "    pred_fig = visualizer.plot_prediction_results(\n",
    "        y_test, y_pred, uncertainties, model_name=best_model_name\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    if 'error' not in results:\n",
    "        metrics = results['test_metrics']\n",
    "        cv_results = results.get('cv_results', {})\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'Test R²': f\"{metrics['r2']:.4f}\",\n",
    "            'Test RMSE': f\"{metrics['rmse']:.4f}\",\n",
    "            'Test MAE': f\"{metrics['mae']:.4f}\",\n",
    "            'CV R² Mean': f\"{cv_results.get('r2', {}).get('mean', 0):.4f}\",\n",
    "            'CV R² Std': f\"{cv_results.get('r2', {}).get('std', 0):.4f}\",\n",
    "            'Performance': results.get('performance_summary', {}).get('performance_category', 'unknown').title()\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if best_model_name:\n",
    "    print(f\"\\n1. BEST PERFORMING MODEL: {best_model_name}\")\n",
    "    print(f\"   - Test R²: {best_r2:.4f}\")\n",
    "    \n",
    "    if best_r2 >= 0.9:\n",
    "        print(\"   - EXCELLENT performance - ready for production\")\n",
    "    elif best_r2 >= 0.8:\n",
    "        print(\"   - GOOD performance - suitable for most applications\")\n",
    "    elif best_r2 >= 0.6:\n",
    "        print(\"   - FAIR performance - may need improvement\")\n",
    "    else:\n",
    "        print(\"   - POOR performance - requires significant improvement\")\n",
    "\n",
    "# Check if ensemble is best\n",
    "if 'Ensemble' in model_results and 'error' not in model_results['Ensemble']:\n",
    "    ensemble_r2 = model_results['Ensemble']['test_metrics']['r2']\n",
    "    print(f\"\\n2. ENSEMBLE MODEL PERFORMANCE:\")\n",
    "    print(f\"   - R²: {ensemble_r2:.4f}\")\n",
    "    \n",
    "    # Compare with individual models\n",
    "    individual_r2s = []\n",
    "    for name, results in model_results.items():\n",
    "        if name != 'Ensemble' and 'error' not in results:\n",
    "            individual_r2s.append(results['test_metrics']['r2'])\n",
    "    \n",
    "    if individual_r2s:\n",
    "        max_individual_r2 = max(individual_r2s)\n",
    "        if ensemble_r2 > max_individual_r2:\n",
    "            print(\"   - Ensemble OUTPERFORMS individual models\")\n",
    "            print(\"   - RECOMMENDATION: Use ensemble for production\")\n",
    "        else:\n",
    "            print(\"   - Individual models perform better\")\n",
    "            print(\"   - RECOMMENDATION: Use best individual model\")\n",
    "\n",
    "print(f\"\\n3. UNCERTAINTY ESTIMATION:\")\n",
    "uncertainty_models = []\n",
    "for name, results in model_results.items():\n",
    "    if 'uncertainty_metrics' in results:\n",
    "        uncertainty_models.append(name)\n",
    "\n",
    "if uncertainty_models:\n",
    "    print(f\"   - Models with uncertainty: {', '.join(uncertainty_models)}\")\n",
    "    print(\"   - RECOMMENDATION: Use uncertainty for risk assessment\")\n",
    "else:\n",
    "    print(\"   - No uncertainty metrics available\")\n",
    "    print(\"   - RECOMMENDATION: Implement uncertainty estimation\")\n",
    "\n",
    "print(f\"\\n4. FEATURE IMPORTANCE:\")\n",
    "if 'Random Forest' in model_results and 'feature_importance' in model_results['Random Forest']:\n",
    "    top_features = model_results['Random Forest']['feature_importance']['top_10_features'][:3]\n",
    "    print(\"   - Top 3 most important features:\")\n",
    "    for i, feat in enumerate(top_features):\n",
    "        print(f\"     {i+1}. {feat['feature']} ({feat['importance']:.4f})\")\n",
    "    print(\"   - RECOMMENDATION: Focus on these features for model interpretation\")\n",
    "\n",
    "print(f\"\\n5. DEPLOYMENT STRATEGY:\")\n",
    "print(\"   - Use ensemble model for robustness\")\n",
    "print(\"   - Implement real-time monitoring\")\n",
    "print(\"   - Set up model retraining pipeline\")\n",
    "print(\"   - Include uncertainty quantification in alerts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "mimetype": "text/x-python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}